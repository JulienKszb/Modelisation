from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt
#import numpy.linalg as npl


data = load_iris()



for i in range(len(data.data)):
    if data.target[i]==0:
        data.target[i]=-1
    if data.target[i]==2:
        data.target[i]=1
    if data.data[i,0]==4.5 and data.data[i,1]==2.3:
        data.target[i]=1
        
for i in range(len(data.data)):
    if data.target[i]==-1:
        plt.scatter(data.data[i,0],data.data[i,1], c="red")
    if data.target[i]==1:
        plt.scatter(data.data[i,0],data.data[i,1], c="blue")
 

n=len(data.data)    
x=np.array(data.data[:,0:2])
y=np.array(data.target)  

def Hlagrangien(alpha): 
    somme=0
    som=0
    for i in range(n):
        som+=alpha[i]
        for j in range(n):
            somme+=alpha[i]*alpha[j]*y[i]*y[j]*np.dot(x[i],x[j])
    return (-1/2*somme + som) * (-1)


def HJacobien(alpha):
    resultat=np.zeros(n)
    for k in range(n):
        somme = 0
        for i in range(n):
            somme += alpha[i]*y[i]*y[k]*np.dot(x[k],x[i])
        resultat[k]=1-somme
    return resultat*(-1)
    
#vérifier le jacobien avec le TAF

def Gradient(fonction,h=1e-4,alpha_initiale=np.zeros(n)):
    alpha = np.copy(alpha_initiale)
    y=[alpha]
    itermax = 50000

    iter = 0
    while iter<itermax:
        df = fonction(alpha)
        alpha = alpha - h*df
        for i in range(n):
            alpha[i]=np.maximum(alpha[i],0)
        y.append(alpha)
        iter += 1
    listalpha=np.array(y)
    return alpha,listalpha

initial=np.array([0.04804102984299476 ,0.08881706170505126 ,0.05483840279653408 ,0.059207556419582474 ,0.03105170038873609 ,0.02425432743519683 ,0.027169831803622174 ,0.05241018346604288 ,0.06794586366567901 ,0.07813782016639788 ,0.04561281051250355 ,0.03979000763483254 ,0.08250697378944591 ,0.05095653421142033 ,0.03881543755896378 ,0.0 ,0.02425432743519683 ,0.04804102984299476 ,0.05386383272066555 ,0.016003305227034487 ,0.07765053512846345 ,0.02668254676568786 ,0.005811348726315651 ,0.06939951292030129 ,0.03979000763483254 ,0.09512714962065602 ,0.05241018346604288 ,0.054351117758599736 ,0.06503035929725323 ,0.05483840279653408 ,0.0718277322507925 ,0.07765053512846345 ,0.0 ,0.0 ,0.07813782016639788 ,0.07376866654334953 ,0.07328138150541519 ,0.02474161247313127 ,0.05726662212702543 ,0.05872027138164806 ,0.041730941927389575 ,0.06166859918679607 ,0.03590813904971855 ,0.041730941927389575 ,0.016003305227034487 ,0.08250697378944591 ,0.016003305227034487 ,0.048528314880928854 ,0.039302722596898154 ,0.06308942500469615 ,2.9575144548177607e-05 ,0.03789010263817879 ,0.0 ,0.0 ,0.0 ,0.03934375189280109 ,0.05487943209243703 ,0.047107489063028654 ,0.0 ,0.060214949932173537 ,0.0 ,0.0480820591388976 ,0.0 ,0.024782641769034025 ,0.05633308134705978 ,0.008280597352709996 ,0.06701232288571314 ,0.022354422438542902 ,0.0 ,0.013616115192446277 ,0.06944054221620406 ,0.014103400230380484 ,0.0 ,0.014103400230380484 ,0.005852378022218575 ,0.003911443729661847 ,0.0 ,0.0 ,0.031092729684639265 ,0.0179852688154944 ,0.009246961569398008 ,0.009246961569398008 ,0.022354422438542902 ,0.009734246607332518 ,0.07963249871692327 ,0.08448893737790572 ,0.008280597352709996 ,0.0 ,0.06701232288571314 ,0.019926203108051432 ,0.030605444646704946 ,0.03546188330768741 ,0.011675180899889436 ,0.030118159608770206 ,0.03497459826975334 ,0.06070223497010787 ,0.05002299343145453 ,0.018472553853428653 ,0.045166554770471816 ,0.03934375189280109 ,0.05487943209243703 ,0.022354422438542902 ,0.0 ,0.012162465937823821 ,0.010221531645266827 ,0.0 ,0.05778673060168202 ,0.0 ,0.0 ,0.030126365467950885 ,0.03158001472257365 ,0.0 ,0.0 ,0.007306027276841046 ,0.03303366397719613 ,0.03789010263817879 ,0.010221531645266827 ,0.01993440896723209 ,0.0 ,0.0 ,0.006339663060153032 ,0.04565383980840653 ,0.0 ,0.0 ,0.02963908043001654 ,0.0 ,0.007793312314775356 ,0.03546188330768741 ,0.0 ,0.0 ,0.0 ,0.007314233136021646 ,0.0 ,0.001483224399170459 ,0.0 ,0.0 ,0.06555867363109043 ,0.027210861099525162 ,0.0417719712232927 ,0.0 ,0.008280597352709996 ,0.0 ,0.022354422438542902 ,0.012649750975758467 ,0.02963908043001654 ,0.0 ,0.0 ,0.010221531645266827 ,0.07186876154669536 ,0.0480820591388976])

alpha,listalpha=Gradient(HJacobien,1e-4,initial)

for i in range(n):
    print(alpha[i],",")

# Calcul de w
w=np.zeros(2)
for i in range(n):
    w+=alpha[i]*y[i]*x[i]
print("w = ",w)

# Calcul de b
b=0
for i in range(n):
    aux=(y[i]/np.abs(y[i])*(1/y[i]-np.dot(x[i],w)))
    b=max(b,aux)

print("b = ",b)


# Définition du plan


# On a w1*x1+w2*x2+b=0
#yy=(-b-w[0]*xx)/w[1]

xx0=4
yy0=(-b-w[0]*xx0)/w[1]
xx1=8
yy1=(-b-w[0]*xx1)/w[1]


for i in range(n):
    if y[i]==1:
        plt.scatter(x[i,0], x[i,1], c="red" )
    elif y[i]==-1:
        plt.scatter(x[i,0], x[i,1], c="blue" )

plt.plot([xx0, xx1], [yy0, yy1], 'g--', lw=2)